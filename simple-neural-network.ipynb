{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "    \"\"\"\n",
    "    Load the MNIST dataset. Reads the training and testing files and create matrices.\n",
    "    :Expected return:\n",
    "    train_data:the matrix with the training data\n",
    "    test_data: the matrix with the data that will be used for testing\n",
    "    y_train: the matrix consisting of one \n",
    "                        hot vectors on each row(ground truth for training)\n",
    "    y_test: the matrix consisting of one\n",
    "                        hot vectors on each row(ground truth for testing)\n",
    "    \"\"\"\n",
    "    \n",
    "    #load the train files\n",
    "    df = None\n",
    "    \n",
    "    y_train = []\n",
    "\n",
    "    for i in range( 10 ):\n",
    "        tmp = pd.read_csv( 'data/mnist/train%d.txt' % i, header=None, sep=\" \" )\n",
    "        #build labels - one hot vector\n",
    "        hot_vector = [ 1 if j == i else 0 for j in range(0,10) ]\n",
    "        \n",
    "        for j in range( tmp.shape[0] ):\n",
    "            y_train.append( hot_vector )\n",
    "            \n",
    "        #concatenate dataframes by rows    \n",
    "        if i == 0:\n",
    "            df = tmp\n",
    "        else:\n",
    "            df = pd.concat( [df, tmp] )\n",
    "\n",
    "    train_data = df.as_matrix()\n",
    "    y_train = np.array( y_train )\n",
    "    \n",
    "    #load test files\n",
    "    df = None\n",
    "    \n",
    "    y_test = []\n",
    "\n",
    "    for i in range( 10 ):\n",
    "        tmp = pd.read_csv( 'data/mnist/test%d.txt' % i, header=None, sep=\" \" )\n",
    "        #build labels - one hot vector\n",
    "        \n",
    "        hot_vector = [ 1 if j == i else 0 for j in range(0,10) ]\n",
    "        \n",
    "        for j in range( tmp.shape[0] ):\n",
    "            y_test.append( hot_vector )\n",
    "            \n",
    "        #concatenate dataframes by rows    \n",
    "        if i == 0:\n",
    "            df = tmp\n",
    "        else:\n",
    "            df = pd.concat( [df, tmp] )\n",
    "\n",
    "    test_data = df.as_matrix()\n",
    "    y_test = np.array( y_test )\n",
    "    \n",
    "    return train_data, test_data, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar():\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset. Reads the training and testing files and create matrices.\n",
    "    :Expected return:\n",
    "    train_data:the matrix with the training data\n",
    "    test_data: the matrix with the data that will be used for testing\n",
    "    y_train: the matrix consisting of one \n",
    "                        hot vectors on each row(ground truth for training)\n",
    "    y_test: the matrix consisting of one\n",
    "                        hot vectors on each row(ground truth for testing)\n",
    "    \"\"\"\n",
    "    \n",
    "    #load the train files\n",
    "    \n",
    "    y_train = []\n",
    "\n",
    "    for i in range( 1, 6 ):\n",
    "        tmp = open('data/cifar/data_batch_%d' % i, 'rb')\n",
    "        \n",
    "        train_dict = pickle.load(tmp, encoding='bytes')\n",
    "        \n",
    "        if i == 1:\n",
    "            train_data = train_dict[b'data']\n",
    "        else:\n",
    "            train_data = np.vstack((train_data, train_dict[b'data']))\n",
    "        \n",
    "        #build labels - one hot vector\n",
    "        \n",
    "        for j in range( len(train_dict[b'labels'])):\n",
    "            y_train.append([ 1 if k == train_dict[b'labels'][j] else 0 for k in range(0,10) ])\n",
    "\n",
    "    y_train = np.array( y_train )\n",
    "     \n",
    "    y_test = []\n",
    "\n",
    "    tmp = open('data/cifar/test_batch', 'rb')\n",
    "        \n",
    "    test_dict = pickle.load(tmp, encoding='bytes')\n",
    " \n",
    "    test_data = test_dict[b'data']\n",
    "\n",
    "    #build labels - one hot vector\n",
    "                \n",
    "    for j in range( len(test_dict[b'labels']) ):\n",
    "        y_test.append( [ 1 if k == test_dict[b'labels'][j] else 0 for k in range(0,10) ] )\n",
    "\n",
    "    y_test = np.array( y_test )\n",
    "    \n",
    "    return train_data, test_data, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(Set=0):\n",
    "    if Set==0:\n",
    "        X_train, X_test, y_train, y_test = load_mnist()\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = load_cifar()\n",
    "    \n",
    "    X_train = X_train.astype(float)/255\n",
    "    X_test = X_test.astype(float)/255\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use by default ax=1, when the array is 2D\n",
    "#use ax=0 when the array is 1D\n",
    "def softmax( x, ax=1 ):\n",
    "    m = np.max( x, axis=ax, keepdims=True )#max per row\n",
    "    p = np.exp( x - m )\n",
    "    return ( p / np.sum(p,axis=ax,keepdims=True) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#H : input array\n",
    "#activation : integer for activation function selection\n",
    "#returns an array modified by the activation function\n",
    "def activation_function(H, activation): \n",
    "    \n",
    "    if activation==1:\n",
    "        z=np.log(1+np.exp(H))\n",
    "        \n",
    "    elif activation==2:\n",
    "        z=(np.exp(H) - (np.exp(-H)))/(np.exp(H) + (np.exp(-H)))\n",
    "\n",
    "    elif activation==3:\n",
    "        z=np.cos(H)\n",
    "\n",
    "    return z   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#H : input array\n",
    "#activation : integer for activation function selection\n",
    "#returns an array with the derivative of the activation function\n",
    "def activation_derivative(H, activation):\n",
    "    \n",
    "    if activation==1:\n",
    "        z=np.divide(np.exp(H),(1+np.exp(H)))\n",
    "        \n",
    "    elif activation==2:\n",
    "        z=1-np.square((np.exp(H) - (np.exp(-H)))/(np.exp(H) + (np.exp(-H))))\n",
    "\n",
    "    elif activation==3:\n",
    "        z=-np.sin(H)\n",
    "\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#H : input array\n",
    "#returns an array where there is added a column to position 0 with all ones\n",
    "def add_bias(Input):\n",
    "    z = np.hstack((np.ones((Input.shape[0],1) ), Input))\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input : input array\n",
    "#W : weight array\n",
    "#returns an array where each input value is multiplied with the corresponding weight\n",
    "def feed_forward(Input,W):\n",
    "    Input=add_bias(Input)\n",
    "    Z=Input.dot(W.T)\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output : the output of a layer\n",
    "#Input : input from previous layer\n",
    "#W : weight array of a layer\n",
    "#returns the gradient of W\n",
    "def back_propagation(Output,Input,lamda,W):\n",
    "    Input=add_bias(Input)\n",
    "    grad=Output.T.dot(Input) - lamda*W\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t : truth values\n",
    "#y : imput multiplied by weights of the last layer\n",
    "#lamda : regularization parameter\n",
    "#W1,W2 : weights of first and second layer\n",
    "#returns the cost for the values of W1 and W2\n",
    "def cost_function(t,y,lamda,W1,W2):\n",
    "    \n",
    "    max_error = np.max(y, axis=1)\n",
    "    Ew = np.sum(t * y) - np.sum(max_error) - \\\n",
    "    np.sum(np.log(np.sum(np.exp(y - np.array([max_error, ] * y.shape[1]).T), 1))) - \\\n",
    "    (0.5 * lamda) * (np.sum(np.square(W1))+np.sum(np.square(W2)))\n",
    "    \n",
    "    return Ew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t : truth values\n",
    "#X : imput values\n",
    "#lamda : regularization parameter\n",
    "#W1,W2 : weights of first and second layer\n",
    "#activation : integer for activation function selection\n",
    "#returns the cost for the values of W1 and W2\n",
    "def cost_grads(t, X, lamda, W1, W2, activation):\n",
    "    \n",
    "    h = feed_forward(X,W1)\n",
    "    z = activation_function(h,activation)\n",
    "    y = feed_forward(z,W2)\n",
    "    s = softmax(y)\n",
    "    \n",
    "    Ew = cost_function(t,y,lamda,W1,W2)\n",
    "\n",
    "    pred=t-s\n",
    "    # calculate gradients\n",
    "    f=(pred.dot(W2[:,1:]))*activation_derivative(h, activation)    \n",
    "    gradEw1=back_propagation(f,X,lamda,W1)\n",
    "    gradEw2=back_propagation(pred,z,lamda,W2)\n",
    "\n",
    "    return Ew,gradEw1,gradEw2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(t, X, W1init,W2init, options):\n",
    "    \"\"\"inputs :\n",
    "      t: N x K one hot vector indicating the output classes\n",
    "      X: N x D input data vector \n",
    "      W1init: M x D+1 dimensional vector of the initial values of the parameters\n",
    "      W2init: K x M+1 dimensional vector of the initial values of the parameters\n",
    "      options: options(1) is the maximum number of iterations\n",
    "               options(2) is the tolerance\n",
    "               options(3) is the learning rate eta\n",
    "               options(4) is the lamda\n",
    "               options(5) is the activation function to use\n",
    "               options(6) is the batch size\n",
    "               options(7) are the checkpoints at which should report error\n",
    "                #iterations, tolerance, learning rate, lamda, activation, batch size, checkpoints\n",
    "\n",
    "    outputs :\n",
    "      W1,W2: the trained vectors of the parameters\n",
    "      costs: the costs during training\n",
    "      \"\"\"\n",
    "    \n",
    "    W1 = W1init\n",
    "    W2 =W2init\n",
    "\n",
    "    # Maximum number of iteration of gradient ascend\n",
    "    _iter = options[0]\n",
    "\n",
    "    # Tolerance\n",
    "    tol = options[1]\n",
    "\n",
    "    # Learning rate\n",
    "    eta = options[2]\n",
    "    \n",
    "    # Lamda\n",
    "    lamda = options[3]\n",
    "    \n",
    "    # Activation to use\n",
    "    activation = options[4]\n",
    "    \n",
    "    # Batch size\n",
    "    batch_size = options[5]\n",
    "    \n",
    "    # Checkpoints\n",
    "    checkpoints = options[6]\n",
    "\n",
    "    Ewold = -np.inf\n",
    "    costs = []\n",
    "    for i in range( 1, _iter+1 ):\n",
    "        batch_cost=[]\n",
    "        for j in range(0,X.shape[0],batch_size):\n",
    "            \n",
    "            Ew,gradEw1,gradEw2 = cost_grads(t[j:j+batch_size,:], X[j:j+batch_size,:], lamda, W1, W2, activation)\n",
    "            # save cost\n",
    "            batch_cost.append(Ew)\n",
    "            costs.append(Ew)\n",
    "            # Break if you achieve the desired accuracy in the cost function\n",
    "            if np.abs(Ew - Ewold) < tol:\n",
    "                break\n",
    "                # Update parameters based on gradient ascend\n",
    "            W1 = W1 + eta * gradEw1\n",
    "            W2 = W2 + eta * gradEw2\n",
    "            \n",
    "            Ewold = Ew\n",
    "        if(i in checkpoints):\n",
    "            report(W1,W2,i,options)\n",
    "    return W1,W2, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns the prediction of the class based on weights, input and activation function\n",
    "def predict(W1,W2, X_test,activation):\n",
    "    \n",
    "    h = feed_forward(X_test,W1)\n",
    "    z = activation_function(h,activation)\n",
    "    y = feed_forward(z,W2)\n",
    "    s = softmax(y)\n",
    "\n",
    "    prediction = np.argmax(s, 1)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prints a report with the neural netowrk parameters and accuracy\n",
    "def report(W1,W2,iteration,options):\n",
    "    print(\"------------------------------------------------\")\n",
    "    print(\"Hidden Layers : \" + str(W1.shape[0]))\n",
    "    print(\"Activation Function Hidden Layer : \" + str(options[4]))\n",
    "    pred = predict(W1,W2, X_test,options[4])\n",
    "    print(\"Accuracy on test dataset : \"+str(np.mean( pred == np.argmax(y_test,1)) ))\n",
    "    print(\"Iterations : \"+ str(iteration))\n",
    "    print(\"Learning rate : \" + str(options[2]))\n",
    "    print(\"Lamda : \"+str(options[3]))\n",
    "    print(\"Batch Size : \"+str(options[5]))\n",
    "    print(\"------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradcheck(W1init,W2init, X, t, lamda,activation):\n",
    "    \n",
    "    W1 = np.random.rand(*W1init.shape)\n",
    "    W2 = np.random.rand(*W2init.shape)\n",
    "    epsilon = 1e-6\n",
    "    \n",
    "    _list = np.random.randint(X.shape[0], size=5)\n",
    "    x_sample = np.array(X[_list, :])\n",
    "    t_sample = np.array(t[_list, :])\n",
    "    \n",
    "    Ew, gradEw1,gradEw2 = cost_grads(t_sample,x_sample, lamda,W1,W2,activation)\n",
    "\n",
    "    numericalGrad1 = np.zeros(gradEw1.shape)\n",
    "    numericalGrad2 = np.zeros(gradEw2.shape)\n",
    "    \n",
    "    for k in range(numericalGrad1.shape[0]):\n",
    "        for d in range(numericalGrad1.shape[1]):\n",
    "            \n",
    "            #add epsilon to the w[k,d]\n",
    "            w_tmp = np.copy(W1)\n",
    "            w_tmp[k, d] += epsilon\n",
    "            e_plus,_,_ = cost_grads(t_sample,x_sample, lamda,w_tmp,W2,activation)\n",
    "\n",
    "            #subtract epsilon to the w[k,d]\n",
    "            w_tmp = np.copy(W1)\n",
    "            w_tmp[k, d] -= epsilon\n",
    "            e_minus,_,_  = cost_grads(t_sample,x_sample, lamda,w_tmp,W2,activation)\n",
    "            \n",
    "            #approximate gradient ( E[ w[k,d] + theta ] - E[ w[k,d] - theta ] ) / 2*e\n",
    "            numericalGrad1[k, d] = (e_plus - e_minus) / (2 * epsilon)\n",
    "        \n",
    "    \n",
    "    for k in range(numericalGrad2.shape[0]):\n",
    "        for d in range(numericalGrad2.shape[1]):\n",
    "            \n",
    "            #add epsilon to the w[k,d]\n",
    "            w_tmp = np.copy(W2)\n",
    "            w_tmp[k, d] += epsilon\n",
    "            e_plus,_,_ = cost_grads(t_sample,x_sample, lamda,W1,w_tmp,activation)\n",
    "\n",
    "            #subtract epsilon to the w[k,d]\n",
    "            w_tmp = np.copy(W2)\n",
    "            w_tmp[k, d] -= epsilon\n",
    "            e_minus,_,_  = cost_grads(t_sample,x_sample, lamda,W1,w_tmp,activation)\n",
    "            \n",
    "            #approximate gradient ( E[ w[k,d] + theta ] - E[ w[k,d] - theta ] ) / 2*e\n",
    "            numericalGrad2[k, d] = (e_plus - e_minus) / (2 * epsilon)\n",
    "    \n",
    "\n",
    "    return ( gradEw1,gradEw2, numericalGrad1,numericalGrad2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lefteris\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:32: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "C:\\Users\\Lefteris\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:55: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = load_data()\n",
    "# N of X\n",
    "N, D = X_train.shape\n",
    "\n",
    "K = 10\n",
    "\n",
    "M = 100\n",
    "np.random.seed(0)\n",
    "# initialize w for the gradient ascent\n",
    "W1init = np.random.randn(M, D+1)*0.01\n",
    "W2init = np.random.randn(K, M+1)*0.01\n",
    "\n",
    "# regularization parameter\n",
    "lamda = 0.001\n",
    "\n",
    "# options for gradient descent\n",
    "options = [500, 1e-6, 10/N, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mYour backward propagation works perfectly fine! difference = 4.12289836112819e-08\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "gradEw1,gradEw2, numericalGrad1,numericalGrad2 = gradcheck(W1init,W2init, X_train, y_train, lamda,options[3])\n",
    "\n",
    "numerator1 = np.linalg.norm(gradEw1 - numericalGrad1)  \n",
    "numerator2 = np.linalg.norm(gradEw2 - numericalGrad2)                                   \n",
    "\n",
    "denominator1 = np.linalg.norm(gradEw1) + np.linalg.norm(numericalGrad1)  \n",
    "denominator2 = np.linalg.norm(gradEw2) + np.linalg.norm(numericalGrad2)\n",
    "\n",
    "difference = (numerator1 +numerator2)/ (denominator1  + denominator2)                                        # Step 3'\n",
    "\n",
    "if difference > 1e-6:\n",
    "    print(\"\\033[93m\" + \"There is a mistake in the backward propagation! difference = \" + str(difference) + \"\\033[0m\")\n",
    "else:\n",
    "    print(\"\\033[92m\" + \"Your backward propagation works perfectly fine! difference = \" + str(difference) + \"\\033[0m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lefteris\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:32: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "C:\\Users\\Lefteris\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:55: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST Dataset\n",
      "***********************************************\n",
      "------------------------------------------------\n",
      "Hidden Layers : 100\n",
      "Activation Function Hidden Layer : 1\n",
      "Accuracy on test dataset : 0.9203\n",
      "Iterations : 300\n",
      "Learning rate : 0.00016666666666666666\n",
      "Lamda : 0.001\n",
      "Batch Size : 200\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "Hidden Layers : 100\n",
      "Activation Function Hidden Layer : 1\n",
      "Accuracy on test dataset : 0.9497\n",
      "Iterations : 500\n",
      "Learning rate : 0.00016666666666666666\n",
      "Lamda : 0.001\n",
      "Batch Size : 200\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "Hidden Layers : 100\n",
      "Activation Function Hidden Layer : 1\n",
      "Accuracy on test dataset : 0.961\n",
      "Iterations : 700\n",
      "Learning rate : 0.00016666666666666666\n",
      "Lamda : 0.001\n",
      "Batch Size : 200\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "Hidden Layers : 100\n",
      "Activation Function Hidden Layer : 2\n",
      "Accuracy on test dataset : 0.9578\n",
      "Iterations : 300\n",
      "Learning rate : 0.00016666666666666666\n",
      "Lamda : 0.001\n",
      "Batch Size : 200\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "Hidden Layers : 100\n",
      "Activation Function Hidden Layer : 2\n",
      "Accuracy on test dataset : 0.9702\n",
      "Iterations : 500\n",
      "Learning rate : 0.00016666666666666666\n",
      "Lamda : 0.001\n",
      "Batch Size : 200\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "Hidden Layers : 100\n",
      "Activation Function Hidden Layer : 2\n",
      "Accuracy on test dataset : 0.9735\n",
      "Iterations : 700\n",
      "Learning rate : 0.00016666666666666666\n",
      "Lamda : 0.001\n",
      "Batch Size : 200\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "Hidden Layers : 100\n",
      "Activation Function Hidden Layer : 3\n",
      "Accuracy on test dataset : 0.9728\n",
      "Iterations : 300\n",
      "Learning rate : 0.00016666666666666666\n",
      "Lamda : 0.001\n",
      "Batch Size : 200\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "Hidden Layers : 100\n",
      "Activation Function Hidden Layer : 3\n",
      "Accuracy on test dataset : 0.9752\n",
      "Iterations : 500\n",
      "Learning rate : 0.00016666666666666666\n",
      "Lamda : 0.001\n",
      "Batch Size : 200\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "Hidden Layers : 100\n",
      "Activation Function Hidden Layer : 3\n",
      "Accuracy on test dataset : 0.9759\n",
      "Iterations : 700\n",
      "Learning rate : 0.00016666666666666666\n",
      "Lamda : 0.001\n",
      "Batch Size : 200\n",
      "------------------------------------------------\n",
      "***********************************************\n",
      "------------------------------------------------\n",
      "Hidden Layers : 200\n",
      "Activation Function Hidden Layer : 1\n",
      "Accuracy on test dataset : 0.9016\n",
      "Iterations : 300\n",
      "Learning rate : 0.00016666666666666666\n",
      "Lamda : 0.001\n",
      "Batch Size : 200\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "Hidden Layers : 200\n",
      "Activation Function Hidden Layer : 1\n",
      "Accuracy on test dataset : 0.9378\n",
      "Iterations : 500\n",
      "Learning rate : 0.00016666666666666666\n",
      "Lamda : 0.001\n",
      "Batch Size : 200\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "Hidden Layers : 200\n",
      "Activation Function Hidden Layer : 1\n",
      "Accuracy on test dataset : 0.9556\n",
      "Iterations : 700\n",
      "Learning rate : 0.00016666666666666666\n",
      "Lamda : 0.001\n",
      "Batch Size : 200\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "Hidden Layers : 200\n",
      "Activation Function Hidden Layer : 2\n",
      "Accuracy on test dataset : 0.9505\n",
      "Iterations : 300\n",
      "Learning rate : 0.00016666666666666666\n",
      "Lamda : 0.001\n",
      "Batch Size : 200\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "Hidden Layers : 200\n",
      "Activation Function Hidden Layer : 2\n",
      "Accuracy on test dataset : 0.9694\n",
      "Iterations : 500\n",
      "Learning rate : 0.00016666666666666666\n",
      "Lamda : 0.001\n",
      "Batch Size : 200\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "Hidden Layers : 200\n",
      "Activation Function Hidden Layer : 2\n",
      "Accuracy on test dataset : 0.9758\n",
      "Iterations : 700\n",
      "Learning rate : 0.00016666666666666666\n",
      "Lamda : 0.001\n",
      "Batch Size : 200\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "Hidden Layers : 200\n",
      "Activation Function Hidden Layer : 3\n",
      "Accuracy on test dataset : 0.9764\n",
      "Iterations : 300\n",
      "Learning rate : 0.00016666666666666666\n",
      "Lamda : 0.001\n",
      "Batch Size : 200\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "Hidden Layers : 200\n",
      "Activation Function Hidden Layer : 3\n",
      "Accuracy on test dataset : 0.9779\n",
      "Iterations : 500\n",
      "Learning rate : 0.00016666666666666666\n",
      "Lamda : 0.001\n",
      "Batch Size : 200\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "Hidden Layers : 200\n",
      "Activation Function Hidden Layer : 3\n",
      "Accuracy on test dataset : 0.9777\n",
      "Iterations : 700\n",
      "Learning rate : 0.00016666666666666666\n",
      "Lamda : 0.001\n",
      "Batch Size : 200\n",
      "------------------------------------------------\n",
      "***********************************************\n",
      "------------------------------------------------\n",
      "Hidden Layers : 300\n",
      "Activation Function Hidden Layer : 1\n",
      "Accuracy on test dataset : 0.892\n",
      "Iterations : 300\n",
      "Learning rate : 0.00016666666666666666\n",
      "Lamda : 0.001\n",
      "Batch Size : 200\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "Hidden Layers : 300\n",
      "Activation Function Hidden Layer : 1\n",
      "Accuracy on test dataset : 0.9379\n",
      "Iterations : 500\n",
      "Learning rate : 0.00016666666666666666\n",
      "Lamda : 0.001\n",
      "Batch Size : 200\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "Hidden Layers : 300\n",
      "Activation Function Hidden Layer : 1\n",
      "Accuracy on test dataset : 0.9539\n",
      "Iterations : 700\n",
      "Learning rate : 0.00016666666666666666\n",
      "Lamda : 0.001\n",
      "Batch Size : 200\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "Hidden Layers : 300\n",
      "Activation Function Hidden Layer : 2\n",
      "Accuracy on test dataset : 0.9441\n",
      "Iterations : 300\n",
      "Learning rate : 0.00016666666666666666\n",
      "Lamda : 0.001\n",
      "Batch Size : 200\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "Hidden Layers : 300\n",
      "Activation Function Hidden Layer : 2\n",
      "Accuracy on test dataset : 0.9647\n",
      "Iterations : 500\n",
      "Learning rate : 0.00016666666666666666\n",
      "Lamda : 0.001\n",
      "Batch Size : 200\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "Hidden Layers : 300\n",
      "Activation Function Hidden Layer : 2\n",
      "Accuracy on test dataset : 0.9712\n",
      "Iterations : 700\n",
      "Learning rate : 0.00016666666666666666\n",
      "Lamda : 0.001\n",
      "Batch Size : 200\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "Hidden Layers : 300\n",
      "Activation Function Hidden Layer : 3\n",
      "Accuracy on test dataset : 0.9748\n",
      "Iterations : 300\n",
      "Learning rate : 0.00016666666666666666\n",
      "Lamda : 0.001\n",
      "Batch Size : 200\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "Hidden Layers : 300\n",
      "Activation Function Hidden Layer : 3\n",
      "Accuracy on test dataset : 0.9784\n",
      "Iterations : 500\n",
      "Learning rate : 0.00016666666666666666\n",
      "Lamda : 0.001\n",
      "Batch Size : 200\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "Hidden Layers : 300\n",
      "Activation Function Hidden Layer : 3\n",
      "Accuracy on test dataset : 0.9792\n",
      "Iterations : 700\n",
      "Learning rate : 0.00016666666666666666\n",
      "Lamda : 0.001\n",
      "Batch Size : 200\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = load_data()\n",
    "print(\"MNIST Dataset\")\n",
    "for i in range(100,400,100):\n",
    "    print(\"***********************************************\")\n",
    "    for activation in range (1,4):\n",
    "        N, D = X_train.shape\n",
    "        K = 10\n",
    "\n",
    "        M = i\n",
    "        np.random.seed(0)\n",
    "                        # initialize w for the gradient ascent\n",
    "        W1init = np.random.randn(M, D+1)*0.01\n",
    "        W2init = np.random.randn(K, M+1)*0.01\n",
    "\n",
    "        iterations = 700 # number of epochs\n",
    "        tolerance = 1e-6 # tolerance\n",
    "        learning_rate = 10/N # learning rate\n",
    "        lamda = 0.001 # regularization parameter    \n",
    "        batch_size = 200  # batch size for gradient ascent\n",
    "        checkpoints = [300,500,700] # epochs at which error will be reported\n",
    "        \n",
    "        # options for gradient descent\n",
    "        #iterations, tolerance, learning rate, lamda, activation, batch size, checkpoints\n",
    "        options = [iterations, tolerance, learning_rate, lamda, activation, batch_size, checkpoints]\n",
    "        \n",
    "        W1,W2, costs=train(y_train, X_train, W1init ,W2init, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR-10 Dataset\n",
      "***********************************************\n",
      "------------------------------------------------\n",
      "Hidden Layers : 100\n",
      "Activation Function Hidden Layer : 1\n",
      "Accuracy on test dataset : 0.4815\n",
      "Iterations : 150\n",
      "Learning rate : 4e-05\n",
      "Lamda : 0.01\n",
      "Batch Size : 100\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "Hidden Layers : 100\n",
      "Activation Function Hidden Layer : 1\n",
      "Accuracy on test dataset : 0.495\n",
      "Iterations : 200\n",
      "Learning rate : 4e-05\n",
      "Lamda : 0.01\n",
      "Batch Size : 100\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "Hidden Layers : 100\n",
      "Activation Function Hidden Layer : 1\n",
      "Accuracy on test dataset : 0.5051\n",
      "Iterations : 250\n",
      "Learning rate : 4e-05\n",
      "Lamda : 0.01\n",
      "Batch Size : 100\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "Hidden Layers : 100\n",
      "Activation Function Hidden Layer : 2\n",
      "Accuracy on test dataset : 0.5\n",
      "Iterations : 150\n",
      "Learning rate : 4e-05\n",
      "Lamda : 0.01\n",
      "Batch Size : 100\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "Hidden Layers : 100\n",
      "Activation Function Hidden Layer : 2\n",
      "Accuracy on test dataset : 0.5147\n",
      "Iterations : 200\n",
      "Learning rate : 4e-05\n",
      "Lamda : 0.01\n",
      "Batch Size : 100\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "Hidden Layers : 100\n",
      "Activation Function Hidden Layer : 2\n",
      "Accuracy on test dataset : 0.5255\n",
      "Iterations : 250\n",
      "Learning rate : 4e-05\n",
      "Lamda : 0.01\n",
      "Batch Size : 100\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "Hidden Layers : 100\n",
      "Activation Function Hidden Layer : 3\n",
      "Accuracy on test dataset : 0.5198\n",
      "Iterations : 150\n",
      "Learning rate : 4e-05\n",
      "Lamda : 0.01\n",
      "Batch Size : 100\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "Hidden Layers : 100\n",
      "Activation Function Hidden Layer : 3\n",
      "Accuracy on test dataset : 0.5241\n",
      "Iterations : 200\n",
      "Learning rate : 4e-05\n",
      "Lamda : 0.01\n",
      "Batch Size : 100\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "Hidden Layers : 100\n",
      "Activation Function Hidden Layer : 3\n",
      "Accuracy on test dataset : 0.5244\n",
      "Iterations : 250\n",
      "Learning rate : 4e-05\n",
      "Lamda : 0.01\n",
      "Batch Size : 100\n",
      "------------------------------------------------\n",
      "***********************************************\n",
      "------------------------------------------------\n",
      "Hidden Layers : 200\n",
      "Activation Function Hidden Layer : 1\n",
      "Accuracy on test dataset : 0.4781\n",
      "Iterations : 150\n",
      "Learning rate : 4e-05\n",
      "Lamda : 0.01\n",
      "Batch Size : 100\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "Hidden Layers : 200\n",
      "Activation Function Hidden Layer : 1\n",
      "Accuracy on test dataset : 0.4908\n",
      "Iterations : 200\n",
      "Learning rate : 4e-05\n",
      "Lamda : 0.01\n",
      "Batch Size : 100\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "Hidden Layers : 200\n",
      "Activation Function Hidden Layer : 1\n",
      "Accuracy on test dataset : 0.5023\n",
      "Iterations : 250\n",
      "Learning rate : 4e-05\n",
      "Lamda : 0.01\n",
      "Batch Size : 100\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "Hidden Layers : 200\n",
      "Activation Function Hidden Layer : 2\n",
      "Accuracy on test dataset : 0.5131\n",
      "Iterations : 150\n",
      "Learning rate : 4e-05\n",
      "Lamda : 0.01\n",
      "Batch Size : 100\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "Hidden Layers : 200\n",
      "Activation Function Hidden Layer : 2\n",
      "Accuracy on test dataset : 0.5248\n",
      "Iterations : 200\n",
      "Learning rate : 4e-05\n",
      "Lamda : 0.01\n",
      "Batch Size : 100\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "Hidden Layers : 200\n",
      "Activation Function Hidden Layer : 2\n",
      "Accuracy on test dataset : 0.5319\n",
      "Iterations : 250\n",
      "Learning rate : 4e-05\n",
      "Lamda : 0.01\n",
      "Batch Size : 100\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "Hidden Layers : 200\n",
      "Activation Function Hidden Layer : 3\n",
      "Accuracy on test dataset : 0.5297\n",
      "Iterations : 150\n",
      "Learning rate : 4e-05\n",
      "Lamda : 0.01\n",
      "Batch Size : 100\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "Hidden Layers : 200\n",
      "Activation Function Hidden Layer : 3\n",
      "Accuracy on test dataset : 0.5381\n",
      "Iterations : 200\n",
      "Learning rate : 4e-05\n",
      "Lamda : 0.01\n",
      "Batch Size : 100\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "Hidden Layers : 200\n",
      "Activation Function Hidden Layer : 3\n",
      "Accuracy on test dataset : 0.5349\n",
      "Iterations : 250\n",
      "Learning rate : 4e-05\n",
      "Lamda : 0.01\n",
      "Batch Size : 100\n",
      "------------------------------------------------\n",
      "***********************************************\n",
      "------------------------------------------------\n",
      "Hidden Layers : 300\n",
      "Activation Function Hidden Layer : 1\n",
      "Accuracy on test dataset : 0.4754\n",
      "Iterations : 150\n",
      "Learning rate : 4e-05\n",
      "Lamda : 0.01\n",
      "Batch Size : 100\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "Hidden Layers : 300\n",
      "Activation Function Hidden Layer : 1\n",
      "Accuracy on test dataset : 0.4904\n",
      "Iterations : 200\n",
      "Learning rate : 4e-05\n",
      "Lamda : 0.01\n",
      "Batch Size : 100\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "Hidden Layers : 300\n",
      "Activation Function Hidden Layer : 1\n",
      "Accuracy on test dataset : 0.5036\n",
      "Iterations : 250\n",
      "Learning rate : 4e-05\n",
      "Lamda : 0.01\n",
      "Batch Size : 100\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "Hidden Layers : 300\n",
      "Activation Function Hidden Layer : 2\n",
      "Accuracy on test dataset : 0.5121\n",
      "Iterations : 150\n",
      "Learning rate : 4e-05\n",
      "Lamda : 0.01\n",
      "Batch Size : 100\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "Hidden Layers : 300\n",
      "Activation Function Hidden Layer : 2\n",
      "Accuracy on test dataset : 0.5276\n",
      "Iterations : 200\n",
      "Learning rate : 4e-05\n",
      "Lamda : 0.01\n",
      "Batch Size : 100\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "Hidden Layers : 300\n",
      "Activation Function Hidden Layer : 2\n",
      "Accuracy on test dataset : 0.5379\n",
      "Iterations : 250\n",
      "Learning rate : 4e-05\n",
      "Lamda : 0.01\n",
      "Batch Size : 100\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "Hidden Layers : 300\n",
      "Activation Function Hidden Layer : 3\n",
      "Accuracy on test dataset : 0.5306\n",
      "Iterations : 150\n",
      "Learning rate : 4e-05\n",
      "Lamda : 0.01\n",
      "Batch Size : 100\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "Hidden Layers : 300\n",
      "Activation Function Hidden Layer : 3\n",
      "Accuracy on test dataset : 0.5366\n",
      "Iterations : 200\n",
      "Learning rate : 4e-05\n",
      "Lamda : 0.01\n",
      "Batch Size : 100\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "Hidden Layers : 300\n",
      "Activation Function Hidden Layer : 3\n",
      "Accuracy on test dataset : 0.5373\n",
      "Iterations : 250\n",
      "Learning rate : 4e-05\n",
      "Lamda : 0.01\n",
      "Batch Size : 100\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = load_data(1)\n",
    "print(\"CIFAR-10 Dataset\")\n",
    "for i in range(100,400,100):\n",
    "    print(\"***********************************************\")\n",
    "    for activation in range (1,4):\n",
    "        N, D = X_train.shape\n",
    "        K = 10\n",
    "\n",
    "        M = i\n",
    "        np.random.seed(0)\n",
    "                        # initialize w for the gradient ascent\n",
    "        W1init = np.random.randn(M, D+1)*0.01\n",
    "        W2init = np.random.randn(K, M+1)*0.01\n",
    "\n",
    "        iterations = 250 # number of epochs\n",
    "        tolerance = 1e-6 # tolerance\n",
    "        learning_rate = 2/N # learning rate\n",
    "        lamda = 0.01 # regularization parameter    \n",
    "        batch_size = 100  # batch size for gradient ascent\n",
    "        checkpoints = [150,200,250] # epochs at which error will be reported\n",
    "        \n",
    "        # options for gradient descent\n",
    "        #iterations, tolerance, learning rate, lamda, activation, batch size, checkpoints\n",
    "        options = [iterations, tolerance, learning_rate, lamda, activation, batch_size, checkpoints]\n",
    "        \n",
    "        W1,W2, costs=train(y_train, X_train, W1init ,W2init, options)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
